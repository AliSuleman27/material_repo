{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Imbalance\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter\n",
    "\n",
    "# Models\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import (VotingClassifier, BaggingClassifier, RandomForestClassifier, \n",
    "                              AdaBoostClassifier, GradientBoostingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, roc_curve, roc_auc_score, mean_squared_error, \n",
    "                             mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()\n",
    "sns.countplot(x='target', data=df, palette='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnderSampling\n",
    "y = df.pop('target')\n",
    "nm = NearMiss()\n",
    "x_res, y_res = nm.fit_resample(df, y)\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))\n",
    "\n",
    "# Oversampling\n",
    "smk = SMOTETomek(random_state=42)\n",
    "xdata,ydata=smk.fit_resample(data,y)\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(ydata)))\n",
    "\n",
    "# After Any Resampling is done\n",
    "df = xdata\n",
    "df['target_col_name'] = ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "def correlation(dataset,threshold):\n",
    "    col_corr = set()\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range (i):\n",
    "            if abs(corr_matrix.iloc[i,j])>threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(df,0.65)\n",
    "len(set(corr_features))\n",
    "df.drop(corr_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essenstial Plots\n",
    "sns.countplot(x='Survived', data=df)\n",
    "sns.boxplot(x='Survived', y='Fare', data=df)\n",
    "sns.barplot(x='Gender', y='Fare', data=df)\n",
    "sns.histplot(df['Age'], kde=True)\n",
    "sns.lineplot(x='Age', y='Fare', data=df)\n",
    "sns.scatterplot(x='Age', y='Fare', hue='Survived', data=df)\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='Blues')\n",
    "df.groupby('Age')['Fare'].sum().plot(kind='area')\n",
    "conf_matrix = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection Syntax\n",
    "X = df.drop(columns=['Level'])\n",
    "y = df['Level']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n",
    "y_train_pred = knn.predict(X_train_final)\n",
    "train_accuracy = accuracy_score(y_train_final, y_train_pred)\n",
    "y_val_pred = knn.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)#,algorithm='kd_tree') #uncomment kd_tree to use its algorithm\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc = accuracy_score(y_pred,y_test)\n",
    "print(acc*100)\n",
    "\n",
    "# DTree Classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy')# or gini, ccp_alpha=0.015 max_depth=? max_leaf_samples)\n",
    "clf.fit(X_train_final, y_train_final)\n",
    "y_train_pred = clf.predict(X_train_final)\n",
    "train_accuracy = accuracy_score(y_train_final, y_train_pred)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Adaboost\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "model = AdaBoostClassifier(estimator=base_estimator,n_estimators=20)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# xgboost\n",
    "xgb_model = XGBClassifier(random_state=0,use_label_encoder=False,eval_metric='logloss')\n",
    "xgb_model.fit(X_train_main,y_train_main)\n",
    "xgb_train_acc = xgb_model.score(X_train_main,y_train_main)\n",
    "xgb_test_acc = xgb_model.score(X_test,y_test)\n",
    "\n",
    "#Random Forest\n",
    "model = RandomForestClassifier(n_estimators=15,max_depth=2)\n",
    "model.fit(X_train,y_train) \n",
    "\n",
    "# Voting Classifier\n",
    "estimators=[('dt', model 1), ('knn', model2), ('rf',model3), ('xgb', model4)]\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='hard' // 'soft')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "accuracy = voting_clf.score(X_test, y_test)\n",
    "\n",
    "# XGBoost\n",
    "xgb = GradientBoostingClassifier(random_state=42,max_depth=10,n_estimators=10)\n",
    "xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1])\n",
    "rf_precision = precision_score(y_test, ypred)\n",
    "rf_recall = recall_score(y_test, ypred)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, knn.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, color='red', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='blue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "\n",
    "knn_scores =[]\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "    knn.fit(X_train_cv, y_train_cv)\n",
    "    knn_scores.append(accuracy_score(y_test_cv, knn.predict(X_test_cv)))\n",
    "knn_avg_score = sum(knn_scores) / len(knn_scores)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(f'TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Essential Functions for preprocessing\n",
    "\n",
    "- df.head()\n",
    "- df.tail()\n",
    "- df.shape\n",
    "- df.columns\n",
    "- df.info()\n",
    "- df.describe()\n",
    "- df.nunique()\n",
    "\n",
    "- for col in df.columns:\n",
    "\tprint('------------------------------')\n",
    "\tprint(df['col'].unique())\n",
    "\n",
    "- for col in df.columns:\n",
    "\tprint('------------------------------')\n",
    "\tprint(df['col'].value_counts)\n",
    "\n",
    "- df.isnull().sum()\n",
    "\n",
    "- df.dropna()\n",
    "- df.fillna()\n",
    "- df.fillna(df.mean(), inplace=True)\n",
    "- df.fillna(df.median(), inplace=True)\n",
    "- df.fillna(df.mode(), inplace=True)\n",
    "- df.fillna(df.pad(), inplace=True)\n",
    "- df[colname].fillna(df[colname].mean(),inplace=True)\n",
    "- df.fillna(method='bfill')\n",
    "- df.fillna(method='ffill')\n",
    "- df[col].fillna(method='bfill or ffil')\n",
    "- df.duplicated(keep=False).sum()\n",
    "- df.drop_duplicates(keep='first')\n",
    "\n",
    "- df.drop('col', axis=1, inplace=True)\n",
    "- df.drop(axis=0, index=1, inplace=True)\n",
    "\n",
    "- df.insert(index, 'col', data)\n",
    "- df.append(df2, ignore_index=True)\n",
    "\n",
    "df3 = pd.concat([df1, df2], axis=0, ignore_index=True) -> Concatenate vertically (adding rows)\n",
    "df_combined = pd.concat([df1, df2], axis=1) -> Concatenate horizontally (adding columns)\n",
    "df_merged = pd.merge(df1, df2, how='inner', on='key') -> Merge DataFrames on the 'key' column\n",
    "\n",
    "- df.reset_index(drop=False, inplace=True)\n",
    "- df.set_index('col')\n",
    "- df.sort_values('col', inplace=True)\n",
    "- df['col'][df['col']==value] == new_value\n",
    "- df['col']\n",
    "- df[['col1', 'col2']]\n",
    "- column = df.pop('col')\n",
    "\n",
    "- df['colname'].replace('?', np.nan, inplace=True)\n",
    "- df['Bare_Nuclei'] = df['Bare_Nuclei'].astype('float64')\n",
    "- def fillValue(value): if value == 'High':return 3 elif value == 'Medium':return 2 elif value == 'Low':return 1\n",
    "- df['Level'] = df['Level'].apply(fillValue)\n",
    "\n",
    "# One Hot Encoding\n",
    "df = pd.get_dummies(df,drop_first=True)\n",
    "\n",
    "# MinMax Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cols = ['Fare','Age']\n",
    "scaler = MinMaxScaler()\n",
    "df[cols] = scaler.fit_transform(df[cols])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
